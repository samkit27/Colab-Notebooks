{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Versions:\n* v9: ColorJitter transformation added **[0.896]**\n* v10: Changed the dataset to [this one](https://www.kaggle.com/shonenkov/melanoma-merged-external-data-512x512-jpeg) with external data. **[0.894]**\n* v11: Switched to [another dataset](https://www.kaggle.com/nroman/melanoma-external-malignant-256/) which I've created by myself. Also switched from StratifiedKFold to GroupKFold **[0.916]**\n* v12: Switched to efficientnet-b1 **[0.919]**\n* v13: Using meta featues: sex and age **[0.918]**\n* v14: anatom_site_general_challenge meta feature added as one-hot encoded matrix **[0.923]**\n* v16: Fixed OOF - now it contains only data from original training dataset, without extarnal data. Also switched back to StratifiedKFold. Added DrawHair augmentation. **[0.909]**\n* v18: Too many things were changed at the same time. All experiments should have only one small change each, so it would be easy to understand how changes affect the result. Said that I rolled back everything, keeping only OOF fix, to make sure it work.\n* v19: Added 'Hair' augmentation. OOF rework posponed untill the best time, since there is some bug in my code for it. **[0.925]**\n* v20: Advanced Hair Augmentation technique used. Read more about it here: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/159176 **[0.923]**\n* v21: Microscope augmentation added instead of Cutout. Read more here: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/159476"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch torchtoolbox","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nCollecting torchtoolbox\n  Downloading torchtoolbox-0.1.4.1-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 2.7 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.2.0.34)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\nCollecting lmdb\n  Downloading lmdb-0.98.tar.gz (869 kB)\n\u001b[K     |████████████████████████████████| 869 kB 14.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.22.2.post1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\nBuilding wheels for collected packages: efficientnet-pytorch, lmdb\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=f0ac611b2972f6d304f4947e402f65dae24bc0ed2d3ed09351b695990b3806c3\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n  Building wheel for lmdb (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=273147 sha256=e60d83b8531212c83dd90674e1092a52ba12da783fb9061b563be7f2bcf673d2\n  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\nSuccessfully built efficientnet-pytorch lmdb\nInstalling collected packages: efficientnet-pytorch, lmdb, torchtoolbox\nSuccessfully installed efficientnet-pytorch-0.6.3 lmdb-0.98 torchtoolbox-0.1.4.1\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchtoolbox.transform as transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport cv2\nimport time\nimport datetime\nimport warnings\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom efficientnet_pytorch import EfficientNet\n%matplotlib inline","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least fixing some random seeds. \n# It is still impossible to make results 100% reproducible when using GPU\nwarnings.simplefilter('ignore')\ntorch.manual_seed(47)\nnp.random.seed(47)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, imfolder: str, train: bool = True, transforms = None, meta_features = None):\n        \"\"\"\n        Class initialization\n        Args:\n            df (pd.DataFrame): DataFrame with data description\n            imfolder (str): folder with images\n            train (bool): flag of whether a training dataset is being initialized or testing one\n            transforms: image transformation method to be applied\n            meta_features (list): list of features with meta information, such as sex and age\n            \n        \"\"\"\n        self.df = df\n        self.imfolder = imfolder\n        self.transforms = transforms\n        self.train = train\n        self.meta_features = meta_features\n        \n    def __getitem__(self, index):\n        im_path = os.path.join(self.imfolder, self.df.iloc[index]['image_name'] + '.jpg')\n        x = cv2.imread(im_path)\n        meta = np.array(self.df.iloc[index][self.meta_features].values, dtype=np.float32)\n\n        if self.transforms:\n            x = self.transforms(x)\n            \n        if self.train:\n            y = self.df.iloc[index]['target']\n            return (x, meta), y\n        else:\n            return (x, meta)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    \nclass Net(nn.Module):\n    def __init__(self, arch, n_meta_features: int):\n        super(Net, self).__init__()\n        self.arch = arch\n        if 'ResNet' in str(arch.__class__):\n            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n        if 'EfficientNet' in str(arch.__class__):\n            self.arch._fc = nn.Linear(in_features=1408, out_features=500, bias=True)\n        self.meta = nn.Sequential(nn.Linear(n_meta_features, 500),\n                                  nn.BatchNorm1d(500),\n                                  nn.ReLU(),\n                                  nn.Dropout(p=0.2),\n                                  nn.Linear(500, 250),  # FC layer output will have 250 features\n                                  nn.BatchNorm1d(250),\n                                  nn.ReLU(),\n                                  nn.Dropout(p=0.2))\n        self.ouput = nn.Linear(500 + 250, 1)\n        \n    def forward(self, inputs):\n        \"\"\"\n        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n        Which applies sigmoid for us when calculating a loss\n        \"\"\"\n        x, meta = inputs\n        cnn_features = self.arch(x)\n        meta_features = self.meta(meta)\n        features = torch.cat((cnn_features, meta_features), dim=1)\n        output = self.ouput(features)\n        return output","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdvancedHairAugmentation:\n    \"\"\"\n    Impose an image of a hair to the target image\n\n    Args:\n        hairs (int): maximum number of hairs to impose\n        hairs_folder (str): path to the folder with hairs images\n    \"\"\"\n\n    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n        self.hairs = hairs\n        self.hairs_folder = hairs_folder\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to draw hairs on.\n\n        Returns:\n            PIL Image: Image with drawn hairs.\n        \"\"\"\n        n_hairs = random.randint(0, self.hairs)\n        \n        if not n_hairs:\n            return img\n        \n        height, width, _ = img.shape  # target image width and height\n        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n        \n        for _ in range(n_hairs):\n            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n\n            h_height, h_width, _ = hair.shape  # hair image width and height\n            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n\n            # Creating a mask and inverse mask\n            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n            mask_inv = cv2.bitwise_not(mask)\n\n            # Now black-out the area of hair in ROI\n            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n            # Take only region of hair from hair image.\n            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n\n            # Put hair in ROI and modify the target image\n            dst = cv2.add(img_bg, hair_fg)\n\n            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n                \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Microscope:\n    \"\"\"\n    Cutting out the edges around the center circle of the image\n    Imitating a picture, taken through the microscope\n\n    Args:\n        p (float): probability of applying an augmentation\n    \"\"\"\n\n    def __init__(self, p: float = 0.5):\n        self.p = p\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to apply transformation to.\n\n        Returns:\n            PIL Image: Image with transformation.\n        \"\"\"\n        if random.random() < self.p:\n            circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8), # image placeholder\n                        (img.shape[0]//2, img.shape[1]//2), # center point of circle\n                        random.randint(img.shape[0]//2 - 3, img.shape[0]//2 + 15), # radius\n                        (0, 0, 0), # color\n                        -1)\n\n            mask = circle - 255\n            img = np.multiply(img, mask)\n        \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(p={self.p})'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = transforms.Compose([\n    AdvancedHairAugmentation(hairs_folder='/kaggle/input/melanoma-hairs/'),\n    transforms.RandomResizedCrop(size=260, scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n    Microscope(p=0.6),\n#     transforms.Cutout(scale=(0.05, 0.007), value=(0, 0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arch = EfficientNet.from_pretrained('efficientnet-b2')  # Going to use efficientnet-b1 NN architecture\n# skf = StratifiedKFold(n_splits=5, random_state=999, shuffle=True)\nskf = GroupKFold(n_splits=5)","execution_count":9,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/checkpoints/efficientnet-b2-8bb594d6.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=36804509.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d46c9e9afd4c58ad2fc11308334ebe"}},"metadata":{}},{"output_type":"stream","text":"\nLoaded pretrained weights for efficientnet-b2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/melanoma-external-malignant-256/train_concat.csv')\ntest_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encoding of anatom_site_general_challenge feature\nconcat = pd.concat([train_df['anatom_site_general_challenge'], test_df['anatom_site_general_challenge']], ignore_index=True)\ndummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix='site')\ntrain_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\ntest_df = pd.concat([test_df, dummies.iloc[train_df.shape[0]:].reset_index(drop=True)], axis=1)\n\n# Sex features\ntrain_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\ntest_df['sex'] = test_df['sex'].map({'male': 1, 'female': 0})\ntrain_df['sex'] = train_df['sex'].fillna(-1)\ntest_df['sex'] = test_df['sex'].fillna(-1)\n\n# Age features\ntrain_df['age_approx'] /= train_df['age_approx'].max()\ntest_df['age_approx'] /= test_df['age_approx'].max()\ntrain_df['age_approx'] = train_df['age_approx'].fillna(0)\ntest_df['age_approx'] = test_df['age_approx'].fillna(0)\n\ntrain_df['patient_id'] = train_df['patient_id'].fillna(0)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"     image_name  patient_id  sex  age_approx anatom_site_general_challenge  \\\n0  ISIC_2637011  IP_7279968  1.0    0.500000                     head/neck   \n1  ISIC_0015719  IP_3075186  0.0    0.500000               upper extremity   \n2  ISIC_0052212  IP_2842074  0.0    0.555556               lower extremity   \n3  ISIC_0068279  IP_6890425  0.0    0.500000                     head/neck   \n4  ISIC_0074268  IP_8723313  0.0    0.611111               upper extremity   \n\n   target  site_anterior torso  site_head/neck  site_lateral torso  \\\n0       0                    0               1                   0   \n1       0                    0               0                   0   \n2       0                    0               0                   0   \n3       0                    0               1                   0   \n4       0                    0               0                   0   \n\n   site_lower extremity  site_oral/genital  site_palms/soles  \\\n0                     0                  0                 0   \n1                     0                  0                 0   \n2                     1                  0                 0   \n3                     0                  0                 0   \n4                     0                  0                 0   \n\n   site_posterior torso  site_torso  site_upper extremity  site_nan  \n0                     0           0                     0         0  \n1                     0           0                     1         0  \n2                     0           0                     0         0  \n3                     0           0                     0         0  \n4                     0           0                     1         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>patient_id</th>\n      <th>sex</th>\n      <th>age_approx</th>\n      <th>anatom_site_general_challenge</th>\n      <th>target</th>\n      <th>site_anterior torso</th>\n      <th>site_head/neck</th>\n      <th>site_lateral torso</th>\n      <th>site_lower extremity</th>\n      <th>site_oral/genital</th>\n      <th>site_palms/soles</th>\n      <th>site_posterior torso</th>\n      <th>site_torso</th>\n      <th>site_upper extremity</th>\n      <th>site_nan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_2637011</td>\n      <td>IP_7279968</td>\n      <td>1.0</td>\n      <td>0.500000</td>\n      <td>head/neck</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015719</td>\n      <td>IP_3075186</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>upper extremity</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0052212</td>\n      <td>IP_2842074</td>\n      <td>0.0</td>\n      <td>0.555556</td>\n      <td>lower extremity</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0068279</td>\n      <td>IP_6890425</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>head/neck</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0074268</td>\n      <td>IP_8723313</td>\n      <td>0.0</td>\n      <td>0.611111</td>\n      <td>upper extremity</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\nmeta_features.remove('anatom_site_general_challenge')","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = MelanomaDataset(df=test_df,\n                       imfolder='/kaggle/input/melanoma-external-malignant-256/test/test/', \n                       train=False,\n                       transforms=train_transform,\n                       meta_features=meta_features)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10  # Number of epochs to run\nmodel_path = 'model.pth'  # Path and filename to save model to\nes_patience = 3  # Early Stopping patience - for how many epochs with no improvements to wait\nTTA = 3 # Test Time Augmentation rounds\n\noof = np.zeros((len(train_df), 1))  # Out Of Fold predictions\npreds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)  # Predictions for test test\n\n# We stratify by target value, thus, according to sklearn StratifiedKFold documentation\n# We can fill `X` with zeroes of corresponding length to use it as a placeholder\n# since we only need `y` to stratify the data\n# for fold, (train_idx, val_idx) in enumerate(skf.split(X=np.zeros(len(train_df)), y=train_df['target']), 1):\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X=np.zeros(len(train_df)), y=train_df['target'], groups=train_df['patient_id'].tolist()), 1):\n    print('=' * 20, 'Fold', fold, '=' * 20)\n    \n    best_val = None  # Best validation score within this fold\n    patience = es_patience  # Current patience counter\n    arch = EfficientNet.from_pretrained('efficientnet-b2')\n    model = Net(arch=arch, n_meta_features=len(meta_features))  # New model for each fold\n    model = model.to(device)\n    \n    \n    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n    criterion = nn.BCEWithLogitsLoss()\n    \n    train = MelanomaDataset(df=train_df.iloc[train_idx].reset_index(drop=True), \n                            imfolder='/kaggle/input/melanoma-external-malignant-256/train/train/', \n                            train=True, \n                            transforms=train_transform,\n                            meta_features=meta_features)\n    val = MelanomaDataset(df=train_df.iloc[val_idx].reset_index(drop=True), \n                            imfolder='/kaggle/input/melanoma-external-malignant-256/train/train/', \n                            train=True, \n                            transforms=test_transform,\n                            meta_features=meta_features)\n    \n    train_loader = DataLoader(dataset=train, batch_size=32, shuffle=True, num_workers=2)\n    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n    \n    for epoch in range(epochs):\n        start_time = time.time()\n        correct = 0\n        epoch_loss = 0\n        model.train()\n        \n        for x, y in train_loader:\n            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n            y = torch.tensor(y, device=device, dtype=torch.float32)\n            optim.zero_grad()\n            z = model(x)\n            loss = criterion(z, y.unsqueeze(1))\n            loss.backward()\n            optim.step()\n            pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n            epoch_loss += loss.item()\n        train_acc = correct / len(train_idx)\n\n        model.eval()  # switch model to the evaluation mode\n        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for j, (x_val, y_val) in enumerate(val_loader):\n                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n                z_val = model(x_val)\n                val_pred = torch.sigmoid(z_val)\n                val_preds[j*x_val[0].shape[0]:j*x_val[0].shape[0] + x_val[0].shape[0]] = val_pred\n            val_acc = accuracy_score(train_df.iloc[val_idx]['target'].values, torch.round(val_preds.cpu()))\n            val_roc = roc_auc_score(train_df.iloc[val_idx]['target'].values, val_preds.cpu())\n            \n            print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n            epoch + 1, \n            epoch_loss, \n            train_acc, \n            val_acc, \n            val_roc, \n            str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n            \n            scheduler.step(val_roc)\n            # During the first iteration (first epoch) best validation is set to None\n            if not best_val:\n                best_val = val_roc  # So any validation roc_auc we have is the best one for now\n                torch.save(model, model_path)  # Saving the model\n                continue\n                \n            if val_roc >= best_val:\n                best_val = val_roc\n                patience = es_patience  # Resetting patience since we have new best validation accuracy\n                torch.save(model, model_path)  # Saving current best model\n            else:\n                patience -= 1\n                if patience == 0:\n                    print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n                    break\n                \n    model = torch.load(model_path)  # Loading best model of this fold\n    model.eval()  # switch model to the evaluation mode\n    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n    with torch.no_grad():\n        # Predicting on validation set once again to obtain data for OOF\n        for j, (x_val, y_val) in enumerate(val_loader):\n            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n            z_val = model(x_val)\n            val_pred = torch.sigmoid(z_val)\n            val_preds[j*x_val[0].shape[0]:j*x_val[0].shape[0] + x_val[0].shape[0]] = val_pred\n        oof[val_idx] = val_preds.cpu().numpy()\n        \n        # Predicting on test set\n        for _ in range(TTA):\n            for i, x_test in enumerate(test_loader):\n                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n                z_test = model(x_test)\n                z_test = torch.sigmoid(z_test)\n                preds[i*x_test[0].shape[0]:i*x_test[0].shape[0] + x_test[0].shape[0]] += z_test\n        preds /= TTA\n        \n    del train, val, train_loader, val_loader, x, y, x_val, y_val\n    gc.collect()\n    \npreds /= skf.n_splits","execution_count":18,"outputs":[{"output_type":"stream","text":"==================== Fold 1 ====================\nLoaded pretrained weights for efficientnet-b2\nEpoch 001: | Loss: 122.055 | Train acc: 0.960 | Val acc: 0.945 | Val roc_auc: 0.963 | Training time: 0:07:25\nEpoch 002: | Loss: 99.755 | Train acc: 0.969 | Val acc: 0.949 | Val roc_auc: 0.970 | Training time: 0:07:25\nEpoch 003: | Loss: 95.158 | Train acc: 0.970 | Val acc: 0.924 | Val roc_auc: 0.954 | Training time: 0:07:24\nEpoch 004: | Loss: 88.104 | Train acc: 0.973 | Val acc: 0.943 | Val roc_auc: 0.951 | Training time: 0:07:22\nEpoch     4: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 005: | Loss: 71.785 | Train acc: 0.978 | Val acc: 0.943 | Val roc_auc: 0.965 | Training time: 0:07:26\nEarly stopping. Best Val roc_auc: 0.970\n==================== Fold 2 ====================\nLoaded pretrained weights for efficientnet-b2\nEpoch 001: | Loss: 131.459 | Train acc: 0.955 | Val acc: 0.967 | Val roc_auc: 0.968 | Training time: 0:07:25\nEpoch 002: | Loss: 108.880 | Train acc: 0.964 | Val acc: 0.977 | Val roc_auc: 0.968 | Training time: 0:07:24\nEpoch 003: | Loss: 97.722 | Train acc: 0.970 | Val acc: 0.976 | Val roc_auc: 0.971 | Training time: 0:07:32\nEpoch 004: | Loss: 95.094 | Train acc: 0.969 | Val acc: 0.974 | Val roc_auc: 0.971 | Training time: 0:07:29\nEpoch 005: | Loss: 92.052 | Train acc: 0.970 | Val acc: 0.973 | Val roc_auc: 0.969 | Training time: 0:07:29\nEpoch 006: | Loss: 88.113 | Train acc: 0.972 | Val acc: 0.976 | Val roc_auc: 0.969 | Training time: 0:07:25\nEpoch     6: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 007: | Loss: 70.303 | Train acc: 0.977 | Val acc: 0.973 | Val roc_auc: 0.975 | Training time: 0:07:28\nEpoch 008: | Loss: 66.190 | Train acc: 0.979 | Val acc: 0.973 | Val roc_auc: 0.976 | Training time: 0:07:29\nEpoch 009: | Loss: 63.689 | Train acc: 0.979 | Val acc: 0.975 | Val roc_auc: 0.977 | Training time: 0:07:30\nEpoch 010: | Loss: 60.975 | Train acc: 0.980 | Val acc: 0.972 | Val roc_auc: 0.975 | Training time: 0:07:23\n==================== Fold 3 ====================\nLoaded pretrained weights for efficientnet-b2\nEpoch 001: | Loss: 129.533 | Train acc: 0.956 | Val acc: 0.967 | Val roc_auc: 0.959 | Training time: 0:07:24\nEpoch 002: | Loss: 104.513 | Train acc: 0.966 | Val acc: 0.959 | Val roc_auc: 0.953 | Training time: 0:07:23\nEpoch 003: | Loss: 99.212 | Train acc: 0.968 | Val acc: 0.973 | Val roc_auc: 0.968 | Training time: 0:07:24\nEpoch 004: | Loss: 93.666 | Train acc: 0.969 | Val acc: 0.969 | Val roc_auc: 0.966 | Training time: 0:07:28\nEpoch 005: | Loss: 88.793 | Train acc: 0.972 | Val acc: 0.970 | Val roc_auc: 0.967 | Training time: 0:07:26\nEpoch     5: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 006: | Loss: 70.925 | Train acc: 0.977 | Val acc: 0.975 | Val roc_auc: 0.971 | Training time: 0:07:27\nEpoch 007: | Loss: 65.288 | Train acc: 0.979 | Val acc: 0.970 | Val roc_auc: 0.970 | Training time: 0:07:24\nEpoch 008: | Loss: 63.517 | Train acc: 0.979 | Val acc: 0.970 | Val roc_auc: 0.968 | Training time: 0:07:19\nEpoch     8: reducing learning rate of group 0 to 4.0000e-05.\nEpoch 009: | Loss: 57.751 | Train acc: 0.981 | Val acc: 0.972 | Val roc_auc: 0.970 | Training time: 0:07:25\nEarly stopping. Best Val roc_auc: 0.971\n==================== Fold 4 ====================\nLoaded pretrained weights for efficientnet-b2\nEpoch 001: | Loss: 125.615 | Train acc: 0.958 | Val acc: 0.944 | Val roc_auc: 0.964 | Training time: 0:07:10\nEpoch 002: | Loss: 101.349 | Train acc: 0.968 | Val acc: 0.972 | Val roc_auc: 0.963 | Training time: 0:07:11\nEpoch 003: | Loss: 96.797 | Train acc: 0.969 | Val acc: 0.973 | Val roc_auc: 0.962 | Training time: 0:07:12\nEpoch     3: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 004: | Loss: 72.703 | Train acc: 0.977 | Val acc: 0.973 | Val roc_auc: 0.969 | Training time: 0:07:12\nEpoch 005: | Loss: 68.605 | Train acc: 0.978 | Val acc: 0.973 | Val roc_auc: 0.968 | Training time: 0:07:11\nEpoch 006: | Loss: 65.546 | Train acc: 0.979 | Val acc: 0.973 | Val roc_auc: 0.968 | Training time: 0:07:12\nEpoch     6: reducing learning rate of group 0 to 4.0000e-05.\nEpoch 007: | Loss: 57.381 | Train acc: 0.981 | Val acc: 0.972 | Val roc_auc: 0.969 | Training time: 0:07:10\nEpoch 008: | Loss: 54.694 | Train acc: 0.982 | Val acc: 0.975 | Val roc_auc: 0.970 | Training time: 0:07:11\nEpoch 009: | Loss: 55.058 | Train acc: 0.981 | Val acc: 0.974 | Val roc_auc: 0.969 | Training time: 0:07:12\nEpoch 010: | Loss: 52.616 | Train acc: 0.982 | Val acc: 0.974 | Val roc_auc: 0.970 | Training time: 0:07:15\n==================== Fold 5 ====================\nLoaded pretrained weights for efficientnet-b2\nEpoch 001: | Loss: 124.903 | Train acc: 0.958 | Val acc: 0.968 | Val roc_auc: 0.959 | Training time: 0:07:34\nEpoch 002: | Loss: 103.221 | Train acc: 0.966 | Val acc: 0.967 | Val roc_auc: 0.962 | Training time: 0:07:32\nEpoch 003: | Loss: 95.208 | Train acc: 0.971 | Val acc: 0.967 | Val roc_auc: 0.967 | Training time: 0:07:31\nEpoch 004: | Loss: 90.089 | Train acc: 0.971 | Val acc: 0.965 | Val roc_auc: 0.969 | Training time: 0:07:27\nEpoch 005: | Loss: 87.740 | Train acc: 0.972 | Val acc: 0.966 | Val roc_auc: 0.966 | Training time: 0:07:24\nEpoch 006: | Loss: 84.998 | Train acc: 0.972 | Val acc: 0.976 | Val roc_auc: 0.974 | Training time: 0:07:27\nEpoch 007: | Loss: 80.896 | Train acc: 0.974 | Val acc: 0.968 | Val roc_auc: 0.971 | Training time: 0:07:21\nEpoch 008: | Loss: 78.817 | Train acc: 0.975 | Val acc: 0.969 | Val roc_auc: 0.974 | Training time: 0:07:21\nEpoch     8: reducing learning rate of group 0 to 2.0000e-04.\nEpoch 009: | Loss: 64.072 | Train acc: 0.979 | Val acc: 0.974 | Val roc_auc: 0.976 | Training time: 0:07:20\nEpoch 010: | Loss: 59.368 | Train acc: 0.980 | Val acc: 0.973 | Val roc_auc: 0.975 | Training time: 0:07:20\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OOF: {:.3f}'.format(roc_auc_score(train_df['target'], oof)))","execution_count":19,"outputs":[{"output_type":"stream","text":"OOF: 0.972\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(pd.Series(preds.cpu().numpy().reshape(-1,)));","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving OOF predictions so stacking would be easier\npd.Series(oof.reshape(-1,)).to_csv('oof.csv', index=False)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\nsub['target'] = preds.cpu().numpy().reshape(-1,)\nsub.to_csv('submission.csv', index=False)","execution_count":22,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}